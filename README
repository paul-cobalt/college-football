NEXT STEPS:

There are three parameters that need to be calibrated: k (which might vary over the course of a season), home team advantage, and the amount of "regression to the mean" that happens between seasons.

Fortunately, you can calibrate these parameters by using a loss function. The loss function should be something like "the sum of how surprised the algorithm is by different results." If the algorithm is surprised all the time, then it is bad. If the algorithm is rarely surprised, then it is good. You should be able to implement a form of gradient descent.

You can train on the first N seasons (since it is elo, it takes a while to normalize), and then evaluate the quality of your model on all the seasons up to 2018. Then run it against 2019 to determine the "true" strengths of all the teams on the field last season.

It would also be good to write an internet article about this whole process (although this can wait until you get a bit deeper into it...). Could be really cool for the website. You have the data now so no one can stop you.

Additionally, would be good to write some unit tests if you want this to be something you show to employers.